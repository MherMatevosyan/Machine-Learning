{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network for XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XOR problem is to find an approximation to the function which classifies points (1,1) , (0,0) as positive examples and (0,1) , (1,0) as negative examples. Below is the plot of classified points where blue indicates positive examples and red indicates negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x204b32d3a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([1,0],[1,0],'bo')\n",
    "plt.plot([1,0],[0,1],'ro')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('XOR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is solved by constructing a neural network with 1 hidden layer using only numpy library. Tqdm library is used to print the progress of for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize\n",
    "input_n = 2 # number of input neurons\n",
    "output_n = 1 # number of output neurons\n",
    "number_hidden = 1 # number of hidden layers\n",
    "h1_n = 2  # number of neurons in the first hidden layer\n",
    "h1_w = np.random.random((input_n+1,h1_n)) # generate random weights from input to first hidden layer \n",
    "out_w = np.random.random((h1_n+1,output_n)) # generate random weights from first hidden layer to output \n",
    "learning_rate = 0.75\n",
    "\n",
    "\n",
    "# With this construction network is limited to having only 1 hidden layer. However, it can have any number\n",
    "# of neurons in each layer. Later I am going to modify  it so it can have any number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid acivation function\n",
    "def activation(x): \n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# forward propogation\n",
    "def forwarder(x):\n",
    "    x = np.append(x,1) # add bias\n",
    "    h1 = np.dot(x,h1_w) # propogate weights from input to first hidden layer\n",
    "    h1_active = activation(h1)\n",
    "    h1_active = np.append(h1_active,1) # add bias\n",
    "    out = np.dot(h1_active,out_w) # propogate weights from first hidden layer to output layer\n",
    "    out_active = activation(out)\n",
    "    return list([h1_active,out_active])\n",
    "\n",
    "\n",
    "# Forwarder takes input data and returns the activated outputs of hidden layer and output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# backward propogation\n",
    "def backwarder(x,y):\n",
    "    fwd = forwarder(x)   \n",
    "    error = y-fwd[1] # error from target value\n",
    "    delta_out = error*fwd[1]*(1-fwd[1]) # delta of output neuron\n",
    "    out_dw = learning_rate*delta_out*fwd[0] # Δw of output neuron\n",
    "    \n",
    "    h1_ub = fwd[0]\n",
    "    delta_h1 = h1_ub*(1-h1_ub)*np.dot(out_w,delta_out) # deltas of hidden layer neurons\n",
    "    h1_dw = learning_rate*delta_h1[:-1]*np.append(x,1)[:, np.newaxis]  # Δw-s of hidden layer neurons\n",
    "    return list([out_dw,h1_dw])\n",
    "\n",
    "\n",
    "# Backwarder takes input data and corresponding target data\n",
    "# passes it through forwarder and returns the Δw-s of \n",
    "# hidden layer neurons and output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def epoch(x,y):\n",
    "    out_all_dw  = 0\n",
    "    h1_all_dw = 0\n",
    "    for indep,dep in zip(x,y):\n",
    "        back = backwarder(indep,dep)\n",
    "        out_all_dw += back[0][:, np.newaxis] # adds Δw-s to output neuron\n",
    "        h1_all_dw += back[1] # adds Δw-s to hidden layer neurons\n",
    "    return list([out_all_dw,h1_all_dw])\n",
    "\n",
    "\n",
    "# Epoch does the forwarding and backpropogation for all 4 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def result(x):\n",
    "    res = []\n",
    "    for example in x:\n",
    "        res.append(forwarder(example)[1])\n",
    "    return res\n",
    "\n",
    "\n",
    "# Result returns predictions for all 4 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XOR data points\n",
    "x_train = np.array([[1,1],\n",
    "                    [0,0],\n",
    "                    [0,1],\n",
    "                    [1,0]])\n",
    "y_train = np.array([[1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is training and printing results of the network. Sometimes the network may get stuck in a local minima. If it happens you can observe that loss gets stuck at around 0.5. This happens due to random weights initializations. Run the notebook from initialization cell again to generate new weights and hopefully get past local minima :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre training:\n",
      "[1 1]   Predict: [ 0.76319392]   Target: [1]   Error: [ 0.23680608]\n",
      "[0 0]   Predict: [ 0.75311825]   Target: [1]   Error: [ 0.24688175]\n",
      "[0 1]   Predict: [ 0.75977969]   Target: [0]   Error: [-0.75977969]\n",
      "[1 0]   Predict: [ 0.7585858]   Target: [0]   Error: [-0.7585858]\n",
      "Loss:  [ 1.26974531]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████████████████                                                                                                                                                          | 9921/50000 [00:02<00:11, 3512.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [ 0.0007567]  for 10000-th epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████████████████████████▏                                                                                                                  | 19937/50000 [00:05<00:08, 3535.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [ 0.00035258]  for 20000-th epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                            | 29960/50000 [00:08<00:05, 3543.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [ 0.00022863]  for 30000-th epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                      | 39977/50000 [00:11<00:02, 3543.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [ 0.00016881]  for 40000-th epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:14<00:00, 3543.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post training:\n",
      "[1 1]   Predict: [ 0.9943997]   Target: [1]   Error: [ 0.0056003]\n",
      "[0 0]   Predict: [ 0.99350789]   Target: [1]   Error: [ 0.00649211]\n",
      "[0 1]   Predict: [ 0.0054828]   Target: [0]   Error: [-0.0054828]\n",
      "[1 0]   Predict: [ 0.00548459]   Target: [0]   Error: [-0.00548459]\n",
      "Loss:  [ 0.00013365]\n"
     ]
    }
   ],
   "source": [
    "# pre-training details\n",
    "loss = 0\n",
    "print('Pre training:')\n",
    "for i in range(len(x_train)):\n",
    "    loss +=(y_train[i]-forwarder(x_train[i])[1] )**2\n",
    "    print( x_train[i],'  Predict:',result(x_train)[i],'  Target:',y_train[i],'  Error:',y_train[i]-forwarder(x_train[i])[1])\n",
    "print( 'Loss: ', loss)\n",
    "\n",
    "# train for 50,000 epochs\n",
    "for i in tqdm(range(50000)):\n",
    "    if i%10000==0 and i!=0: # print loss every 10000-th epoch\n",
    "        loss = 0\n",
    "        for j in range(len(x_train)):\n",
    "            loss +=(y_train[j]-forwarder(x_train[j])[1] )**2\n",
    "        print( 'Loss: ', loss,' for '+str(i)+'-th epoch')\n",
    "    e = epoch(x_train,y_train)\n",
    "    out_w += e[0]\n",
    "    h1_w += e[1]\n",
    "\n",
    "    \n",
    "# post_training details    \n",
    "loss = 0\n",
    "print( 'Post training:')\n",
    "for i in range(len(x_train)):\n",
    "    loss +=(y_train[i]-forwarder(x_train[i])[1] )**2\n",
    "    print( x_train[i],'  Predict:',result(x_train)[i],'  Target:',y_train[i],'  Error:',y_train[i]-forwarder(x_train[i])[1])\n",
    "print( 'Loss: ', loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
